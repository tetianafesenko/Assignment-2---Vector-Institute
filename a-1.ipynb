{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical [25 pts]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Utilize ChatGPT (https://chat.openai.com/auth/login) to provide you with example python code of a decision tree classifier for one of the classification sklearn.datasets (e.g., iris/breast cancer, wine) with training, validation, and test splits. This implementation will produce an X_train, X_val, X_test, y_train, y_val and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.90\n",
      "Test accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier and fit it to the training data\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to make predictions on the validation set\n",
    "y_val_pred = clf.predict(X_val)\n",
    "\n",
    "# Evaluate the accuracy of the classifier on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Use the trained classifier to make predictions on the test set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Validation accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Why do we need training/test splits? Why training/validation/test spltis?[5 pts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training/test splits and training/validation/test splits are used in machine learning to evaluate the performance of a model and ensure that it is not overfitting to the data.\n",
    "\n",
    "A training/test split involves dividing a dataset into two parts, with one part used for training the model and the other part used for testing the model's performance. \n",
    "\n",
    "A training/validation/test split involves dividing the dataset into 3 parts: \n",
    "##### 1. Training set which is used to train the model\n",
    "##### 2. Validation set which is used to tune hyperparameters and evaluate the model's performance during training\n",
    "##### 3. Test set which is used to evaluate the final performance of the model after it has been trained and tuned\n",
    "\n",
    "Using a validation set is very important when performing hyperparameter tuning, because it allows you to evaluate the performance of different hyperparameter settings without overfitting the test set. Additionally, by evaluating the model on a separate test set, you can get a more accurate estimate of how well the model will perform in the real world."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) What are the dimensions d of the X_test matrix and what do they mean?  How many data points are in the validation, training, and test sets? (HINT: use ’shape’) [5 pts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of the X_test matrix depend on the number of features used in the machine learning model. Assuming that the test set is stored in a matrix where each row represents a data point and each column represents a feature, the dimensions of the X_test matrix would be (n_test, d), where n_test is the number of data points in the test set and d is the number of features used in the model.\n",
    "\n",
    "The number of data points in the validation, training, and test sets depends on how the data is split. Assuming that the data is split using a 60/20/20 ratio for training, validation, and test sets respectively, and that the data is stored in numpy arrays X_train, y_train, X_val, y_val, X_test, and y_test, we can find the number of data points in each set using the .shape method:\n",
    "\n",
    "X_train.shape[0] and y_train.shape[0] would give the number of data points in the training set\n",
    "X_val.shape[0] and y_val.shape[0] would give the number of data points in the validation set\n",
    "X_test.shape[0] and y_test.shape[0] would give the number of data points in the test set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab2d146ad33dd4f4d63978bea66f53105172270cc7b3023d23cd814b605dc135"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
